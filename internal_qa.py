# -*- coding: utf-8 -*-
"""Internal QA.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KbfXAQFjksL-RMApTrw3wDHKJ26dWyxC
"""

!pip install pdfplumber

!pip install tika

!pip install openai

import pdfplumber
from tika import parser
import re

!pip install pdfreader

from transformers import GPT2TokenizerFast

import pandas as pd

tokenizer = GPT2TokenizerFast.from_pretrained("gpt2")

def count_tokens(text: str) -> int:
    """count the number of tokens in a string"""
    return len(tokenizer.encode(text))

import re
alphabets= "([A-Za-z])"
prefixes = "(Mr|St|Mrs|Ms|Dr)[.]"
suffixes = "(Inc|Ltd|Jr|Sr|Co)"
starters = "(Mr|Mrs|Ms|Dr|Prof|Capt|Cpt|Lt|He\s|She\s|It\s|They\s|Their\s|Our\s|We\s|But\s|However\s|That\s|This\s|Wherever)"
acronyms = "([A-Z][.][A-Z][.](?:[A-Z][.])?)"
websites = "[.](com|net|org|io|gov|edu|me)"
digits = "([0-9])"

def split_into_sentences(text):
    text = " " + text + "  "
    text = text.replace("\n"," ")
    text = re.sub(prefixes,"\\1<prd>",text)
    text = re.sub(websites,"<prd>\\1",text)
    text = re.sub(digits + "[.]" + digits,"\\1<prd>\\2",text)
    if "..." in text: text = text.replace("...","<prd><prd><prd>")
    if "Ph.D" in text: text = text.replace("Ph.D.","Ph<prd>D<prd>")
    text = re.sub("\s" + alphabets + "[.] "," \\1<prd> ",text)
    text = re.sub(acronyms+" "+starters,"\\1<stop> \\2",text)
    text = re.sub(alphabets + "[.]" + alphabets + "[.]" + alphabets + "[.]","\\1<prd>\\2<prd>\\3<prd>",text)
    text = re.sub(alphabets + "[.]" + alphabets + "[.]","\\1<prd>\\2<prd>",text)
    text = re.sub(" "+suffixes+"[.] "+starters," \\1<stop> \\2",text)
    text = re.sub(" "+suffixes+"[.]"," \\1<prd>",text)
    text = re.sub(" " + alphabets + "[.]"," \\1<prd>",text)
    if "”" in text: text = text.replace(".”","”.")
    if "\"" in text: text = text.replace(".\"","\".")
    if "!" in text: text = text.replace("!\"","\"!")
    if "?" in text: text = text.replace("?\"","\"?")
    text = text.replace(".",".<stop>")
    text = text.replace("?","?<stop>")
    text = text.replace("!","!<stop>")
    text = text.replace("<prd>",".")
    sentences = text.split("<stop>")
    sentences = sentences[:-1]
    sentences = [s.strip() for s in sentences]
    return sentences

pdf = pdfplumber.open("0.pdf")
page = pdf.pages[0]
text = page.extract_text()

p =

print(p)

heading = ["Timeline", "Information for Candidates", "Information for Reviewers","Information for Staff"]

content_list = []

count = 0
pdf = 0
title_list = []
for i in range(0,10,1):
  try:
    temp = str(i)+".pdf"
    print(temp)
    for j in range(0,10,1):
      try:
        pdf = pdfplumber.open(temp)
        page = pdf.pages[j]
        text = page.extract_text()

        #title = re.findall('([A-Z]+(?:(?!\s?[A-Z][a-z])\s?[A-Z])+)', a[0])
        content_list += text
        pdf.close()
      except:
        continue
  except:
    continue

token_list = []
for i in content_list:
  temp = count_tokens(i)
  token_list.append(temp)

lst1 = range(len(content_list))
lst2 = range(len(content_list))

print(len(lst1))
print(len(lst2))
print(len(token_list))
print(len(content_list))

df_out = pd.DataFrame(
    {'title': lst1,
     'heading': lst2,
     'content': content_list,
     'tokens': token_list,
    })

df_out.to_csv("temp.csv")

openai.api_key = ""

def reply_open_ai(text):
  # Use the `Completion` class to generate text with GPT-3
    completion = openai.Completion.create(
    engine="text-davinci-003",
    prompt= "Generate 5 questions and answer to the context below" + text,
    max_tokens=3000,
    n=5,
    stop=None,
    temperature=0,
)
    generated_text = completion.choices[0].text
    print("GPT Response: " , generated_text)
    return generated_text

print(text)

questions_and_answers = reply_open_ai(text)



!pip install tiktoken

!pip install openai

import numpy as np
import openai
import pandas as pd
import pickle
import tiktoken

COMPLETIONS_MODEL = "text-davinci-003"
EMBEDDING_MODEL = "text-embedding-ada-002"

openai.api_key = ""

df = pd.read_csv('temp.csv')
df = df.loc[:,["title","heading","content","tokens"]]
#df = df.rename(columns={'Title': 'title', 'Heading': 'heading', 'Content': 'content',"Token": "tokens"})
df = df.set_index(["title", "heading"])
print(f"{len(df)} rows in the data.")

import numpy as np
df1, df2, df3,df4,df5 = np.array_split(df, 5)

print(df1.shape)
print(df2.shape)
print(df3.shape)

def get_embedding(text: str, model: str=EMBEDDING_MODEL) -> list[float]:
    result = openai.Embedding.create(
      model=model,
      input=text
    )
    return result["data"][0]["embedding"]

def compute_doc_embeddings(df: pd.DataFrame) -> dict[tuple[str, str], list[float]]:
    """
    Create an embedding for each row in the dataframe using the OpenAI Embeddings API.

    Return a dictionary that maps between each embedding vector and the index of the row that it corresponds to.
    """
    return {
        idx: get_embedding(r.content) for idx, r in df.iterrows()
    }

def load_embeddings(fname: str) -> dict[tuple[str, str], list[float]]:
    """
    Read the document embeddings and their keys from a CSV.

    fname is the path to a CSV with exactly these named columns:
        "title", "heading", "0", "1", ... up to the length of the embedding vectors.
    """

    df = pd.read_csv(fname, header=0)
    max_dim = max([int(c) for c in df.columns if c != "title" and c != "heading"])
    return {
           (r.title, r.heading): [r[str(i)] for i in range(max_dim + 1)] for _, r in df.iterrows()
    }

#document_embeddings = load_embeddings("https://cdn.openai.com/API/examples/data/olympics_sections_document_embeddings.csv")

# ===== OR, uncomment the below line to recaculate the embeddings from scratch. ========

document_embeddings_1 = compute_doc_embeddings(df1)

document_embeddings_2 = compute_doc_embeddings(df2)

document_embeddings_3 = compute_doc_embeddings(df3)

document_embeddings_4 = compute_doc_embeddings(df4)

document_embeddings_5 = compute_doc_embeddings(df3)



document_embeddings = {}
document_embeddings.update(document_embeddings_1)
document_embeddings.update(document_embeddings_2)
document_embeddings.update(document_embeddings_3)
document_embeddings.update(document_embeddings_4)
document_embeddings.update(document_embeddings_5)

df = pd.read_csv("final_testing.csv")
df = df.loc[:,["title","heading","content","tokens"]]
#df = df.rename(columns={'Title': 'title', 'Heading': 'heading', 'Content': 'content',"Token": "tokens"})
df = df.set_index(["title", "heading"])

with open('final_saved_dictionary.pkl', 'rb') as f:
    loaded_dict = pickle.load(f)

document_embeddings = loaded_dict

# An example embedding:
example_entry = list(document_embeddings.items())[0]
print(f"{example_entry[0]} : {example_entry[1][:5]}... ({len(example_entry[1])} entries)")

print(example_entry)

def vector_similarity(x: list[float], y: list[float]) -> float:
    """
    Returns the similarity between two vectors.

    Because OpenAI Embeddings are normalized to length 1, the cosine similarity is the same as the dot product.
    """
    return np.dot(np.array(x), np.array(y))

def order_document_sections_by_query_similarity(query: str, contexts: dict[(str, str), np.array]) -> list[(float, (str, str))]:
    """
    Find the query embedding for the supplied query, and compare it against all of the pre-calculated document embeddings
    to find the most relevant sections.

    Return the list of document sections, sorted by relevance in descending order.
    """
    query_embedding = get_embedding(query)

    document_similarities = sorted([
        (vector_similarity(query_embedding, doc_embedding), doc_index) for doc_index, doc_embedding in contexts.items()
    ], reverse=True)

    return document_similarities

order_document_sections_by_query_similarity("", document_embeddings)[:5]

order_document_sections_by_query_similarity("When is the labour holiday?", document_embeddings)[:5]

MAX_SECTION_LEN = 500
SEPARATOR = "\n* "
ENCODING = "gpt2"  # encoding for text-davinci-003

encoding = tiktoken.get_encoding(ENCODING)
separator_len = len(encoding.encode(SEPARATOR))

f"Context separator contains {separator_len} tokens"

def construct_prompt(question: str, context_embeddings: dict, df: pd.DataFrame) -> str:
    """
    Fetch relevant
    """
    most_relevant_document_sections = order_document_sections_by_query_similarity(question, context_embeddings)

    chosen_sections = []
    chosen_sections_len = 0
    chosen_sections_indexes = []

    for _, section_index in most_relevant_document_sections:
        # Add contexts until we run out of space.
        document_section = df.loc[section_index]

        chosen_sections_len += document_section.tokens + separator_len
        if chosen_sections_len > MAX_SECTION_LEN:
            break

        chosen_sections.append(SEPARATOR + document_section.content.replace("\n", " "))
        chosen_sections_indexes.append(str(section_index))

    # Useful diagnostic information
    print(f"Selected {len(chosen_sections)} document sections:")
    print("\n".join(chosen_sections_indexes))

    header = """Answer the question as truthfully as possible using the provided context, and if the answer is not contained within the text below, say "I don't know."\n\nContext:\n"""

    return header + "".join(chosen_sections) + "\n\n Q: " + question + "\n A:"

prompt = construct_prompt(
    "When is the labour holiday?",
    document_embeddings,
    df
)

print("===\n", prompt)

COMPLETIONS_API_PARAMS = {
    # We use temperature of 0.0 because it gives the most predictable, factual answer.
    "temperature": 0.0,
    "max_tokens": 300,
    "model": COMPLETIONS_MODEL,
}
def answer_query_with_context(
    query: str,
    df: pd.DataFrame,
    document_embeddings: dict[(str, str), np.array],
    show_prompt: bool = False
) -> str:
    prompt = construct_prompt(
        query,
        document_embeddings,
        df
    )

    if show_prompt:
        print(prompt)

    response = openai.Completion.create(
                prompt=prompt,
                **COMPLETIONS_API_PARAMS
            )

    return response["choices"][0]["text"].strip(" \n")

answer_query_with_context("when the final decision made?", df, document_embeddings)

fquery = "what happen of sep2?"
answer = answer_query_with_context(query, df, document_embeddings)

print(f"\nQ: {query}\nA: {answer}")

df.to_csv("final_testing.csv")

with open('final_saved_dictionary.pkl', 'wb') as f:
    pickle.dump(document_embeddings , f)

with open('final_saved_dictionary.pkl', 'rb') as f:
    loaded_dict = pickle.load(f)

df = pd.read_csv("final_testing.csv")
df = df.loc[:,["title","heading","content","tokens"]]
#df = df.rename(columns={'Title': 'title', 'Heading': 'heading', 'Content': 'content',"Token": "tokens"})
df = df.set_index(["title", "heading"])

query = "what is the name of my school?"
answer = answer_query_with_context(query, df, loaded_dict)

print(f"\nQ: {query}\nA: {answer}")

